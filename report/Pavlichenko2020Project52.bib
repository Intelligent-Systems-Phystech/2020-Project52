% Encoding: UTF-8

@Misc{WikiPedia,
  key =  	"WikiPedia",
  keywords =	"wiki php mysql",
  note = 	"http://www.wikipedia.org",
  title =	"{WikiPedia}, a web-based, free-content encyclopedia",
  URL =  	"http://www.wikipedia.org",
}

@Article{Bronstein2017,
  author    = {Michael M. Bronstein and Joan Bruna and Yann LeCun and Arthur Szlam and Pierre Vandergheynst},
  title     = {Geometric Deep Learning: Going beyond Euclidean data},
  journal   = {{IEEE} Signal Processing Magazine},
  year      = {2017},
  volume    = {34},
  number    = {4},
  pages     = {18--42},
  month     = {jul},
  doi       = {10.1109/msp.2017.2693418},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Battaglia2018,
  author      = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
  title       = {Relational inductive biases, deep learning, and graph networks},
  abstract    = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  date        = {2018-06-04},
  eprint      = {http://arxiv.org/abs/1806.01261v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1806.01261v3:PDF},
  keywords    = {cs.LG, cs.AI, stat.ML},
}

@InProceedings{Baldassarre2020GRAPHQAPM,
  author = {Federico Baldassarre and Hossein Azizpour and David Men{\'e}ndez Hurtado and Arne Elofsson},
  title  = {GRAPHQA: PROTEIN MODEL QUALITY ASSESSMENT USING GRAPH CONVOLUTIONAL NETWORK},
  year   = {2020},
}

@Article{Lundstroem2008,
  author    = {Jesper Lundström and Leszek Rychlewski and Janusz Bujnicki and Arne Elofsson},
  title     = {Pcons: A neural-network-based consensus predictor that improves fold recognition},
  journal   = {Protein Science},
  year      = {2008},
  volume    = {10},
  number    = {11},
  pages     = {2354--2362},
  month     = {dec},
  doi       = {10.1110/ps.08501},
  publisher = {Wiley},
}

@Article{Arnold2005,
  author    = {Konstantin Arnold and Lorenza Bordoli and Jürgen Kopp and Torsten Schwede},
  title     = {The {SWISS}-{MODEL} workspace: a web-based environment for protein structure homology modelling},
  journal   = {Bioinformatics},
  year      = {2005},
  volume    = {22},
  number    = {2},
  pages     = {195--201},
  month     = {nov},
  doi       = {10.1093/bioinformatics/bti770},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Wang2017,
  author    = {Sheng Wang and Siqi Sun and Zhen Li and Renyu Zhang and Jinbo Xu},
  title     = {Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model},
  journal   = {{PLOS} Computational Biology},
  year      = {2017},
  volume    = {13},
  number    = {1},
  pages     = {e1005324},
  month     = {jan},
  doi       = {10.1371/journal.pcbi.1005324},
  editor    = {Avner Schlessinger},
  publisher = {Public Library of Science ({PLoS})},
}

@Article{Xu2019,
  author    = {Jinbo Xu},
  title     = {Distance-based protein folding powered by deep learning},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2019},
  volume    = {116},
  number    = {34},
  pages     = {16856--16865},
  month     = {aug},
  doi       = {10.1073/pnas.1821309116},
  publisher = {Proceedings of the National Academy of Sciences},
}

@Article{Wallner2003,
  author    = {Björn Wallner and Arne Elofsson},
  title     = {Can correct protein models be identified?},
  journal   = {Protein Science},
  year      = {2003},
  volume    = {12},
  number    = {5},
  pages     = {1073--1086},
  month     = {may},
  doi       = {10.1110/ps.0236803},
  publisher = {Wiley},
}

@Article{Ray2012,
  author    = {Arjun Ray and Erik Lindahl and Björn Wallner},
  title     = {Improved model quality assessment using {ProQ}2},
  journal   = {{BMC} Bioinformatics},
  year      = {2012},
  volume    = {13},
  number    = {1},
  pages     = {224},
  doi       = {10.1186/1471-2105-13-224},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Uziela2016,
  author    = {Karolis Uziela and Nanjiang Shu and Björn Wallner and Arne Elofsson},
  title     = {{ProQ}3: Improved model quality assessments using Rosetta energy terms},
  journal   = {Scientific Reports},
  year      = {2016},
  volume    = {6},
  number    = {1},
  month     = {oct},
  doi       = {10.1038/srep33509},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Hurtado2018,
  author      = {David Menéndez Hurtado and Karolis Uziela and Arne Elofsson},
  title       = {Deep transfer learning in the assessment of the quality of protein models},
  abstract    = {MOTIVATION: Proteins fold into complex structures that are crucial for their biological functions. Experimental determination of protein structures is costly and therefore limited to a small fraction of all known proteins. Hence, different computational structure prediction methods are necessary for the modelling of the vast majority of all proteins. In most structure prediction pipelines, the last step is to select the best available model and to estimate its accuracy. This model quality estimation problem has been growing in importance during the last decade, and progress is believed to be important for large scale modelling of proteins. The current generation of model quality estimation programs performs well at separating incorrect and good models, but fails to consistently identify the best possible model. State-of-the-art model quality assessment methods use a combination of features that describe a model and the agreement of the model with features predicted from the protein sequence. RESULTS: We first introduce a deep neural network architecture to predict model quality using significantly fewer input features than state-of-the-art methods. Thereafter, we propose a methodology to train the deep network that leverages the comparative structure of the problem. We also show the possibility of applying transfer learning on databases of known protein structures. We demonstrate its viability by reaching state-of-the-art performance using only a reduced set of input features and a coarse description of the models. AVAILABILITY: The code will be freely available for download at github.com/ElofssonLab/ProQ4.},
  date        = {2018-04-17},
  eprint      = {http://arxiv.org/abs/1804.06281v1},
  eprintclass = {q-bio.BM},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1804.06281v1:PDF},
  keywords    = {q-bio.BM},
}

@Article{Derevyanko2018,
  author    = {Georgy Derevyanko and Sergei Grudinin and Yoshua Bengio and Guillaume Lamoureux},
  title     = {Deep convolutional networks for quality assessment of protein folds},
  journal   = {Bioinformatics},
  year      = {2018},
  volume    = {34},
  number    = {23},
  pages     = {4046--4053},
  month     = {jun},
  doi       = {10.1093/bioinformatics/bty494},
  editor    = {Alfonso Valencia},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Pages2019,
  author    = {Guillaume Pag{\`{e}}s and Benoit Charmettant and Sergei Grudinin},
  title     = {Protein model quality assessment using 3D oriented convolutional neural networks},
  journal   = {Bioinformatics},
  year      = {2019},
  volume    = {35},
  number    = {18},
  pages     = {3313--3319},
  month     = {feb},
  doi       = {10.1093/bioinformatics/btz122},
  editor    = {Alfonso Valencia},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Conover2019,
  author    = {Matthew Conover and Max Staples and Dong Si and Miao Sun and Renzhi Cao},
  title     = {{AngularQA}: Protein Model Quality Assessment with {LSTM} Networks},
  year      = {2019},
  month     = {feb},
  doi       = {10.1101/560995},
  publisher = {Cold Spring Harbor Laboratory},
}

@Article{Kipf2016,
  author      = {Thomas N. Kipf and Max Welling},
  title       = {Semi-Supervised Classification with Graph Convolutional Networks},
  abstract    = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  date        = {2016-09-09},
  eprint      = {http://arxiv.org/abs/1609.02907v4},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1609.02907v4:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Book{1994,
  title     = {Encyclopaedia of Mathematics (Set)},
  publisher = {SPRINGER NATURE},
  year      = {1994},
  isbn      = {1556080107},
  date      = {1994-02-28},
  ean       = {9781556080104},
  pagetotal = {5402},
  url       = {https://www.ebook.de/de/product/7146939/encyclopaedia_of_mathematics_set.html},
}

@Book{Mueller1966,
  title     = {Spherical Harmonics},
  publisher = {Springer Berlin Heidelberg},
  year      = {1966},
  author    = {Müller, Claus},
  isbn      = {3540036008},
  date      = {1966-01-01},
  ean       = {9783540036005},
  pagetotal = {52},
  url       = {https://www.ebook.de/de/product/7059214/claus_mueller_spherical_harmonics.html},
}

@Article{Olechnovic2012,
  author    = {Kliment Olechnovi{\v{c}} and Eleonora Kulberkyt{\.{e}} and {\v{C}}eslovas Venclovas},
  title     = {{CAD}-score: A new contact area difference-based function for evaluation of protein structural models},
  journal   = {Proteins: Structure, Function, and Bioinformatics},
  year      = {2012},
  volume    = {81},
  number    = {1},
  pages     = {149--162},
  month     = {sep},
  doi       = {10.1002/prot.24172},
  publisher = {Wiley},
}

@Comment{jabref-meta: databaseType:bibtex;}
