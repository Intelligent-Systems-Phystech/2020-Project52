\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\English
\title
%    [Нелинейное ранжирование результатов разведочного информационного поиска] % Краткое название; не нужно, если полное название влезает в~колонтитул
{Quality prediction of proteins models with spherical convolutions on three-dimensional graphs}
\author
%    [Мамонов~К.\,Р.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
{Nikita Pavlichenko, Sergei Grudinin, Ilia Igashov.} % основной список авторов, выводимый в оглавление
%[Мамонов~К.\,Р.$^1$, Воронцов~К.\,В.$^1$, Еремеев~М.\,А.$^1$] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
%\thanks
%    {Работа выполнена при финансовой поддержке РФФИ, проект \No\,00-00-00000.
%   Научный руководитель:  Стрижов~В.\,В.
%{Задачу поставил:  Воронцов~К.\,В.
%	 Консультант:  Еремеев~М.\,А.}
\email
{pavlichenko.nv@phystech.ru, sergei.grudinin@inria.fr, igashov.i@yandex.ru}
\organization
{$^1$ Moscow Institute of Physics and Technolgy, Moscow, Russia}%; $^2$Организация}
\abstract
{Convolutional neural networks have become very popular in recent years, and, in particular, have found widespread application in computer
vision. Recently, active work has also begun on graph convolutional networks. In general, the graphs, unlike the pictures, are irregular
structures, and in many tasks of learning on graphs sample objects also do not have unified topology. Therefore, the existing operations
of convolution on the graphs are very much simplified, and the task of pulling on the graphs remain open in general. The purpose of this
work is to research new operations of convolution on three-dimensional graphs within the framework of solving the problem of quality
estimation of three-dimensional models of proteins (the problem of regression on the graph nodes). These operations are theoretical mechaincs
inspired methods based on the expancion of a function of spherical coordinates as a linear combination of spherical harmonics. It helps
solve the problem of capturing some local 3D structure of protein residues.
	
	\bigskip
	\textbf{Key words}: \emph {graph convolutional networks, spherical convlutions, three-dimensional graphs learning}.}
\titleEng
{Quality prediction of proteins models with spherical convolutions on three-dimensional graphs}
\authorEng
{Author~F.\,S.$^1$, CoAuthor~F.\,S.$^2$, Name~F.\,S.$^2$}
\organizationEng
{$^1$Moscow Institute of Physics and Technolgy, Moscow, Russia;}
\abstractEng
{This document is an example of paper prepared with \LaTeXe\
	typesetting system and style file \texttt{jmlda.sty}.
	
	\bigskip
	\textbf{Keywords}: \emph{keyword, keyword, more keywords}.}
\begin{document}
	
	\maketitle
	
	\section{Introduction}
	Protein molecules are an important part of any biological form. They determine cellular 
	functions and behavior of various biological and chemical structures. It makes the discovery 
	and prediction of proteins structure one of the most important points of medical, chemical 
	and genetic science researches.

	Molecules of proteins consist of smaller molecules called amino acids. These amino acids
	form a chain that is folded and placed in space. Thus, protein functions are determined
	by their positions in a 3D space. So, having this chain of amino acids we need to identify
	how they are located. There are ways to do this experimentally, but it can be time-consuming,
	expensive and not always possible. To solve these disadvantages, computational algorithms \cite{Arnold2005}\cite{Lundstroem2008}\cite{Xu2019}
	were developed that generate different chain foldings. The problem is that no algorithm is the 
	best one. Some of proteins are better modeled by one algorithm, others by others. Therefore, we 
	are facing the problem of quality assessment (QA) of these protein models.

	This problem has recently got attention from the machine learning community. Various artificial
	intelligence methods were applied such as neural networks \cite{Wallner2003} and support vector machines \cite{Ray2012}\cite{Uziela2016}.
	More recent approaches mostly include deep learning methods \cite{Hurtado2018}\cite{Derevyanko2018}\cite{Pages2019}\cite{Conover2019}. The newest approach is to use graph
	machine learning methods such as Graph Convolutional Networks (GCN) \cite{Baldassarre2020GRAPHQAPM}, where the protein is in some
	way represented as a graph. This work brings the new idea of capturing the 3D structure of this graph 
	to improve the quality of GCN using convlutions based on spherical harmonics.


	\section{Problem statement}
	Consider a 3D model of a protein in space. The protein represents a chain of amino acids rolled up in space. 
	Through dividing space around a protein into cells, for example, by the Voronoi method, we can get a
	3D-graph, the vertices of which are amino acids of protein and edges are carried out between those amino
	acids that are in adjacent cells. Denote the resulting graph by $G = (V, E)$, where verticles $V = (v_1, \ldots, v_n)$
	are a set of amino acids, $E$ are edges. For the $i$-th vertex we denote for $\mathcal{N}(v_i)$ the
	set of its neighbors in a graph $G$ and for $G$ an adjacency matrix $\boldsymbol{A}$:
	$$A_{ij} = \begin{cases}
		1, & (v_i, v_j) \in E \\
		0, & \text{otherwise}.
	\end{cases}$$ 

	Consider that each vertex $v_i$ is described by some real $d$-dimensional vector of attributes 
	$x(v_i) = \boldsymbol{x}_i$. In the simplest case, it can be a one-hot representation of an amino acid type.
	We can form feature matrix $\boldsymbol{X}$ from vectors $x(v_i)$ for every vertex $x_i$.
	Using these data we will solve a regression problem: to predict for each vertex $v_i$ a real number - its
	"score". In other words, how correctly it is placed in the given 3D-model in comparison with the 
	actual conformation of this protein.

	\section{Graph Convolutional Networks}
	We will develop approach for machine learning on graphs using Graph Convolutional Network (GCN) \cite{Kipf2016}.
	The idea of this approach is aggregation of neighbors features for every vertex and forming new feature vectors on the
	layer's output. So, for GCN with $L$ layers, $L \geqslant 2$ we have:
	$$A_{ij} = \begin{cases}
		\boldsymbol{H}^{(0)} = \boldsymbol{X} \\
		\boldsymbol{H}^{(l)} = \sigma\left(\boldsymbol{A} \boldsymbol{H}^{l-1}\boldsymbol{W}^{(l-1)}\right) & \text{for } l \in \{1, \ldots, L - 1\} \\
		\boldsymbol{H}^{(L)} =  \boldsymbol{A} \boldsymbol{H}^{L-1}\boldsymbol{W}^{(L-1)}.
	\end{cases}$$
	In the above $\boldsymbol{H}^{(l)}$ denotes feature matrix at $l$-th layer and $\boldsymbol{W}^{(l)}$ denotes weight matrix. Common choise for activation
	function $\sigma$ is a ReLU activation: $\text{ReLU}(x) := \max(x, 0)$ or an ELU activation: $\text{ELU}(x) := \begin{cases}
		x, & x \geqslant 0 \\
		\alpha(e^x - 1), & x < 0
	\end{cases}$.
	Wheights matrix $\boldsymbol{W}^{(l)}$ are trained by stochastic gradient descend or its modifications.

	We rely on this approach because it has already showed outperformance in protein quality
	 assessment problem\cite{Baldassarre2020GRAPHQAPM}.
	
	The main problem here is that classic GCN is not able to use information about locations of vertices in space. This is because the coordinates in space
	are given ambiguously: the turned or shifted proteins are actually isomorphic. So our impovement is to capture such information from
	sequential structure of a protein.

	\section{Spherical Convolutional Networks}
	\subsection{Spherical harmonics}
	Let us consider a function $f(\Omega) : [0, \pi] \times [0, 2\pi) \rightarrow \mathbb{R}$ — mapping from unit sphere to real
	numbers. Such function might be unknown or its evaluation might be time consuming. So, we want to expance this function as a linear
	combination of simplier functions. The common approach that came from theoretical mechaincs is to use spherical harmonics\cite{Mueller1966}.
	We will use the following form of spherical functions:
	$$
		Y_l^m(\phi, \psi) = \sqrt{\dfrac{(2l + 1)}{4\pi}\dfrac{(l-m)!}{(l+m)!}}P_l^m(\cos \phi)e^{im\psi}, 
	$$
	where $P_l^{m}$ are associated Legendre polynoms\cite{Mueller1966}.

	On the unit sphere, any square-integrable function can thus be expanded as a linear combination of these:
	$$
	f(\phi, \psi) = \sum_{l = 0}^{\infty} \sum_{m=-l}^{m=l}f_l^m Y_l^m(\phi, \psi).
	$$
	We will use this fact to train a function of vertex features using the first few components of this expancion. 
	Since the function is real, the real representation of spherical harmonics can be used:
	$$
	Y_{lm} = \begin{cases}
		\sqrt{2}(-1)^m \Im\left[Y_l^{|m|}\right] & \text{if } m < 0 \\
		Y_l^0 & m = 0 \\
		\sqrt{2}(-1)^m \Re\left[Y_l^{m}\right] & \text{if } m > 0.
	\end{cases}
	$$

	\subsection{Spherical convolution}
	Consider a vertex $v_i$. Since all the amino acids in the protein are connected in a peptide chain, it is easy to
	construct its local coordinate system for the amino acid $v_i$ under consideration — on two dihedral corners,
	which are unambiguously determined from the geometry of the peptide chain. Write the coordinates of all the neighbors of $v_j \in \mathcal{N}(v_i)$
	of the amino acid in the obtained coordinate system. Then proceed to spherical coordinates, and project all vertices onto a unit sphere
	with the center in $v_i$. Now, each vertex $v_j \in \mathcal{N}(v_i)$ can be matched with a pair of angles $\Omega_i^j = (\phi_i^j, \psi_i^j)$ that specify the
	angular position of the projection of the vertex $v_j$ onto a unit sphere in the local coordinate system of $v_i$.

	Now, having an unambiguous orientation for each vertex of the graph, we can introduce the convolution operation. Let us consider a certain 
	matrix function $f(\Omega) : [0, \pi] \times [0, 2\pi) \rightarrow \mathbb{R}^{d \times d'} $ on a single sphere. We can expance it
	into a series on the basis of spherical functions $\{Y_l^m\}_{l,m}$ and leave the first few components:
	$$
		f(\Omega) \approx f_W(\Omega) = \sum_{l=0}^{L}\sum_m \boldsymbol{W}_l^m Y_l^m(\Omega),
	$$
	where $\boldsymbol{W}_l^m$ denotes coefficient matrix in the expancion of matrix function $f$ on the basis of $\{Y_l^m\}_{l,m}$. Then
	we can introduce the spherical convolution operation for the vertex $v_i$ in the following way:
	$$f_W \circ v_i = \sum_{v_j \in \mathcal{N}(v_i)} f_W(\Omega_i^j)x(v_i).$$

	Considering $\boldsymbol{W}_l^m$ matrices to be optimized parameters, we will thus train spherical filters.

	\subsection{Spherical convolution layer}
	Let us have a 3D model of a protein consisting of $N$ amino acids, the $i$-th amino acid is described by the trait vector $\boldsymbol{x}_i \in \mathbb{R}^d$.
	Let us denote all the vertices through the $\boldsymbol{X} \in \mathbb{R}^{N \times d}$ feature matrix. Then one layer of spherical
	convolution is written down as follows:
	$$
		\boldsymbol{X} \longrightarrow \boldsymbol{X}' = \sigma(f_W \circ \boldsymbol{X}) = \sigma\left(\sum_{l,m}Y_l^m(\boldsymbol{A}_\Omega)\boldsymbol{X}\boldsymbol{W}_l^m\right),
	$$
	where $\sigma$ is an activation function, $Y_l^m$ are spherical functions, $\boldsymbol{W}_l^m$ are optimized parameters and $\boldsymbol{A}_\Omega$
	is the adjacency matrix of graph $G$, which cells contains the spherical coordinates of the vertices in the corresponding local coordinate system:
	$$
		[\boldsymbol{A}_\Omega]_{i,j} = \begin{cases}
			\Omega_i^j, & (v_i, v_j) \in E \\
			0 & \text{otherwise}.
		\end{cases}
	$$

	\section{Experiment}
	\subsection{Dataset}
	All models are taken from the from CASP competition data (\url{http://predictioncenter.org}), which is dedicated to the prediction 
	of the 3D structure of proteins by the amino acid sequence. We take data from CASP12 and CASP13 as a test sample, and data from
	earlier CASPs as a training sample.

	For each protein there is one real experimentally obtained 3D structure and many generated 3D structures. Our task is to predict 
	the CAD-score \cite{Olechnovic2012} for each of the generated structures i.e. how similar each of the generated structures is to the real one.
	%\begin{State}
	%    Мотивации и~интерпретации наиболее важны для понимания сути работы.
	%\end{State}
	
	%\begin{Theorem}
	%    Не~менее $90\%$ коллег, заинтересовавшихся Вашей статьёй,
	%    прочитают в~ней не~более~$10\%$ текста.
	%\end{Theorem}
	%
	%\begin{Proof}
	%    Причём это будут именно те~разделы, которые не содержат формул.
	%\end{Proof}
	%
	%\begin{Remark}
	%    Выше показано применение окружений
	%    Def, Theorem, State, Remark, Proof.
	%\end{Remark}
	
	
	%\section{Заключение}
	
	%Желательно, чтобы этот раздел был, причём он не~должен дословно повторять аннотацию.
	%Обычно здесь отмечают,
	%каких результатов удалось добиться,
	%какие проблемы остались открытыми.
	
	
	\bibliographystyle{plain}
	\bibliography{Pavlichenko2020Project52}
	
	% Решение Программного Комитета:
	%\ACCEPTNOTE
	%\AMENDNOTE
	%\REJECTNOTE
\end{document}
